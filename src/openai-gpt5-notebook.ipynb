{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ea0de6",
   "metadata": {},
   "source": [
    "# GPT-5 Model Access Test Notebook (Azure OpenAI / Foundry)\n",
    "\n",
    "This notebook provides minimal test coverage for calling an Azure OpenAI (e.g., GPT-5 placeholder) deployment via three mechanisms:\n",
    "\n",
    "1. Raw HTTPS (Responses API) using `aiohttp`\n",
    "2. OpenAI Python SDK (configured for Azure endpoint)\n",
    "3. Azure Foundry / Inference SDK (`azure-ai-inference`)\n",
    "\n",
    "## Imports & Environment Variables\n",
    "We will import:\n",
    "- `os` for environment access\n",
    "- `dotenv` to load a local `.env` if present\n",
    "- `azure.identity` `DefaultAzureCredential` for Entra ID auth\n",
    "- `aiohttp` for raw REST calls\n",
    "- `openai` SDK for Responses API abstractions\n",
    "- `azure.ai.inference` client for Foundry SDK access (Responses)\n",
    "\n",
    "### Required Environment Variables (must be set before running code cells)\n",
    "- `AZURE_OPENAI_ENDPOINT` (e.g. https://my-resource.openai.azure.com)\n",
    "- `AZURE_OPENAI_API_VERSION` (e.g. 2024-06-01 or latest supported)\n",
    "- `AZURE_OPENAI_MODEL` (deployment or model name)\n",
    "\n",
    "Authentication: We'll use `DefaultAzureCredential()` which chains multiple auth methods. It must have access to the Azure OpenAI resource. A token for scope `https://cognitiveservices.azure.com/.default` will be requested.\n",
    "\n",
    "Run the next cell to set up shared variables and credential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports, environment, and credential\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import aiohttp, asyncio\n",
    "import json\n",
    "import openai\n",
    "# Azure Inference / Foundry SDK (beta)\n",
    "try:\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import UserMessage, SystemMessage, AssistantMessage\n",
    "    _FOUND_CLIENT_TYPE = 'chat'\n",
    "except ImportError:\n",
    "    # Fallback generic client naming if API changes\n",
    "    from azure.ai.inference import Client as ChatCompletionsClient  # type: ignore\n",
    "    from azure.ai.inference.models import UserMessage, SystemMessage, AssistantMessage  # type: ignore\n",
    "    _FOUND_CLIENT_TYPE = 'generic'\n",
    "\n",
    "load_dotenv(override=False)\n",
    "endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')\n",
    "api_version = os.environ.get('AZURE_OPENAI_API_VERSION')\n",
    "model = os.environ.get('AZURE_OPENAI_MODEL')\n",
    "if not all([endpoint, api_version, model]):\n",
    "    raise RuntimeError('One or more required environment variables are missing: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION, AZURE_OPENAI_MODEL')\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token('https://cognitiveservices.azure.com/.default')\n",
    "bearer = token.token\n",
    "print('Environment and credential ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9938da41",
   "metadata": {},
   "source": [
    "## Raw Responses API Test (aiohttp)\n",
    "This cell makes a direct POST call to the Azure OpenAI Responses endpoint.\n",
    "Endpoint pattern: `{AZURE_OPENAI_ENDPOINT}/openai/responses?api-version={AZURE_OPENAI_API_VERSION}`\n",
    "Headers: Bearer token from Entra ID and `Content-Type: application/json`.\n",
    "Payload: a minimal prompt asking the model to reply briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "\n",
    "# Raw API call using aiohttp (SSE streaming version)\n",
    "# This replaces the prior non-streaming call to demonstrate incremental token receipt.\n",
    "# NOTE: This is a minimal illustrative parser for Azure Responses API SSE events.\n",
    "# It accumulates output_text deltas and captures the final completion event.\n",
    "\n",
    "import sys\n",
    "\n",
    "async def stream_raw_responses(prompt: str) -> Dict[str, Any]:\n",
    "    url = f'{endpoint}/openai/responses?api-version={api_version}'\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {bearer}',\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'text/event-stream'\n",
    "    }\n",
    "    payload = {\n",
    "        'model': model,\n",
    "        'input': prompt,\n",
    "        'stream': True  # instruct the service to stream via SSE\n",
    "    }\n",
    "    events: list[dict] = []\n",
    "    accumulated_text: list[str] = []\n",
    "    final_event: dict | None = None\n",
    "\n",
    "    rprint('[bold blue]--- Streaming (SSE) start ---[/bold blue]')\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, headers=headers, data=json.dumps(payload)) as resp:\n",
    "            resp.raise_for_status()\n",
    "            async for raw_line in resp.content:\n",
    "                line = raw_line.decode('utf-8', errors='ignore').strip()\n",
    "                if not line:\n",
    "                    continue  # skip keep-alives / empty\n",
    "                if not line.startswith('data:'):\n",
    "                    continue  # ignore non-data lines\n",
    "                data = line[5:].strip()\n",
    "                if data == '[DONE]':\n",
    "                    rprint('\\n[bold blue]--- Streaming complete marker received ---[/bold blue]')\n",
    "                    break\n",
    "                try:\n",
    "                    evt = json.loads(data)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                events.append(evt)\n",
    "                etype = evt.get('type', '')\n",
    "\n",
    "                # Capture deltas (example type: response.output_text.delta)\n",
    "                if etype.endswith('output_text.delta'):\n",
    "                    delta = evt.get('delta') or ''\n",
    "                    if delta:\n",
    "                        accumulated_text.append(delta)\n",
    "                        # Print delta inline without newline for a streaming feel\n",
    "                        print(delta, end='', flush=True)\n",
    "                elif etype.endswith('output_text.done'):\n",
    "                    print()  # newline after finishing text stream\n",
    "                elif etype == 'response.completed':\n",
    "                    final_event = evt\n",
    "                # Could add reasoning / other event handling here if needed\n",
    "    rprint('[bold blue]--- Streaming (SSE) end ---[/bold blue]')\n",
    "\n",
    "    # Build a synthesized final result resembling the non-streaming shape for downstream logic\n",
    "    synthesized: dict = {\n",
    "        'stream_events_count': len(events),\n",
    "        'output_text': ''.join(accumulated_text) if accumulated_text else None,\n",
    "    }\n",
    "    if final_event:\n",
    "        # Merge some commonly useful fields\n",
    "        synthesized.update({\n",
    "            'id': final_event.get('response', {}).get('id') or final_event.get('id'),\n",
    "            'model': final_event.get('response', {}).get('model') or final_event.get('model'),\n",
    "            'usage': final_event.get('response', {}).get('usage') or final_event.get('usage'),\n",
    "            'reasoning': final_event.get('response', {}).get('reasoning') or final_event.get('reasoning'),\n",
    "        })\n",
    "    return synthesized\n",
    "\n",
    "raw_result = await stream_raw_responses('Tell me about AI and its impact on society.')\n",
    "\n",
    "# --- Robust extraction helpers (unchanged except they now see synthesized output_text) ---\n",
    "\n",
    "def extract_reasoning(resp: dict) -> dict:\n",
    "    reasoning_info = {\n",
    "        'effort': None,\n",
    "        'summary': None,\n",
    "        'reasoning_blocks': []\n",
    "    }\n",
    "    rtop = resp.get('reasoning')\n",
    "    if isinstance(rtop, dict):\n",
    "        reasoning_info['effort'] = rtop.get('effort')\n",
    "        reasoning_info['summary'] = rtop.get('summary')\n",
    "    # Streaming synthesis currently does not capture intermediate reasoning blocks; could be extended.\n",
    "    return reasoning_info\n",
    "\n",
    "reasoning_info = extract_reasoning(raw_result)\n",
    "\n",
    "# Extract token usage (may be None if not provided in final event synthesis)\n",
    "usage = raw_result.get('usage', {}) or {}\n",
    "input_tokens = usage.get('input_tokens')\n",
    "output_tokens = usage.get('output_tokens')\n",
    "reasoning_tokens = (usage.get('output_tokens_details') or {}).get('reasoning_tokens') if usage else None\n",
    "\n",
    "# Print with colors using rich\n",
    "rprint(f\"[bold blue]Token Usage:[/bold blue] [yellow]Input:[/yellow] {input_tokens} | [magenta]Reasoning:[/magenta] {reasoning_tokens} | [cyan]Output:[/cyan] {output_tokens}\")\n",
    "rprint(f\"[bold purple]Reasoning Effort:[/bold purple] [white]{reasoning_info.get('effort')}[/white]\")\n",
    "rprint(f\"[bold purple]Reasoning Summary:[/bold purple] [white]{reasoning_info.get('summary')}[/white]\")\n",
    "rprint(f\"[bold purple]Stream Events Count:[/bold purple] {raw_result.get('stream_events_count')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab3af65",
   "metadata": {},
   "source": [
    "## OpenAI SDK Test (Azure endpoint)\n",
    "Using the `openai` Python SDK configured for Azure. We pass the Azure AD token as a bearer override by setting a custom `api_key` placeholder and injecting the token via default headers. Newer SDK versions allow `azure_endpoint` style configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e72cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI SDK streaming (SSE) Responses API call\n",
    "from rich import print as rprint\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token=bearer,\n",
    ")\n",
    "\n",
    "# We'll stream a slightly longer prompt to observe deltas.\n",
    "prompt = \"Explain in one short sentence what AI is.\"\n",
    "\n",
    "accumulated: list[str] = []\n",
    "final_response = None\n",
    "usage = None\n",
    "reasoning_effort = None\n",
    "reasoning_summary = None\n",
    "stream_events = 0\n",
    "\n",
    "rprint(\"[bold cyan]--- OpenAI SDK Streaming Start ---[/bold cyan]\")\n",
    "try:\n",
    "    # Newer OpenAI SDK pattern: context manager for streaming\n",
    "    with client.responses.stream(model=model, input=prompt) as stream:\n",
    "        for event in stream:\n",
    "            stream_events += 1\n",
    "            etype = getattr(event, \"type\", None)\n",
    "            # Output text deltas\n",
    "            if etype and etype.endswith(\"output_text.delta\"):\n",
    "                delta = getattr(event, \"delta\", \"\") or \"\"\n",
    "                if delta:\n",
    "                    accumulated.append(delta)\n",
    "                    print(delta, end=\"\", flush=True)\n",
    "            elif etype and etype.endswith(\"output_text.done\"):\n",
    "                print()  # newline after final text\n",
    "            elif etype == \"response.completed\":\n",
    "                # The final event exposes the full response in event.response\n",
    "                final_response = getattr(event, \"response\", None)\n",
    "        # Explicit close to release underlying connection\n",
    "        stream.close()\n",
    "except Exception as e:\n",
    "    rprint(f\"[red]Streaming error:[/red] {e}\")\n",
    "\n",
    "rprint(\"[bold cyan]--- OpenAI SDK Streaming End ---[/bold cyan]\")\n",
    "\n",
    "# Synthesize details from final response if available\n",
    "if final_response:\n",
    "    try:\n",
    "        usage = getattr(final_response, \"usage\", None)\n",
    "        reasoning = getattr(final_response, \"reasoning\", None)\n",
    "        if reasoning:\n",
    "            reasoning_effort = getattr(reasoning, \"effort\", None)\n",
    "            reasoning_summary = getattr(reasoning, \"summary\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "final_text = \"\".join(accumulated) if accumulated else None\n",
    "\n",
    "# Print summary\n",
    "if usage:\n",
    "    input_tokens = getattr(usage, \"input_tokens\", None)\n",
    "    output_tokens = getattr(usage, \"output_tokens\", None)\n",
    "    reasoning_tokens = None\n",
    "    # Try nested details if present\n",
    "    try:\n",
    "        details = getattr(usage, \"output_tokens_details\", None)\n",
    "        if details:\n",
    "            reasoning_tokens = getattr(details, \"reasoning_tokens\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    input_tokens = output_tokens = reasoning_tokens = None\n",
    "\n",
    "rprint(f\"[bold green]Final Text:[/bold green] [white]{final_text}[/white]\")\n",
    "rprint(f\"[bold blue]Token Usage:[/bold blue] [yellow]Input:[/yellow] {input_tokens} | [magenta]Reasoning:[/magenta] {reasoning_tokens} | [cyan]Output:[/cyan] {output_tokens}\")\n",
    "rprint(f\"[bold purple]Reasoning Effort:[/bold purple] [white]{reasoning_effort}[/white]\")\n",
    "rprint(f\"[bold purple]Reasoning Summary:[/bold purple] [white]{reasoning_summary}[/white]\")\n",
    "rprint(f\"[bold purple]Stream Events Count:[/bold purple] {stream_events}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundry-model-testing (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
