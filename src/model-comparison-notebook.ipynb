{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6103268",
   "metadata": {},
   "source": [
    "# Model Comparison Notebook (Azure OpenAI Responses API)\n",
    "\n",
    "This notebook compares token consumption and latency across multiple Azure OpenAI (Foundry) model deployments using the modern unified **Responses API** (`POST /openai/v1/responses`) authenticated with Entra ID (`DefaultAzureCredential`). A compatibility flag allows falling back to the legacy deployment-scoped endpoint if needed.\n",
    "\n",
    "## What this notebook does\n",
    "- Sends the *same* prompt to a list of model deployment names (specified via environment variables) using the unified Responses API (`model` provided in the JSON body).\n",
    "- Collects input, output, and reasoning token usage (if available).\n",
    "- Measures request latency (wall clock).\n",
    "- Aggregates results into an ASCII / colorized table (uses `rich`).\n",
    "- Highlights most efficient (lowest total tokens) model and shows relative percentage differences.\n",
    "\n",
    "## Requirements / Assumptions\n",
    "1. You have valid Azure OpenAI model deployments (their deployment names are passed as the `model` value).\n",
    "2. You are already logged in / have a valid environment for `DefaultAzureCredential`.\n",
    "3. Environment variables are set as described in the next cell.\n",
    "4. Responses API returns usage fields (`input_tokens`, `output_tokens`, `total_tokens`, optional reasoning tokens via `reasoning_tokens` or `output_tokens_details.reasoning_tokens`).\n",
    "5. Set `USE_LEGACY_DEPLOYMENT_PATH=True` if your region / API version still requires the older `/openai/deployments/{deployment}/responses?api-version=...` path.\n",
    "\n",
    "If anything is missing, the validation cell will fail fast with remediation guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ba0a0",
   "metadata": {},
   "source": [
    "## Environment Variables & Naming Pattern\n",
    "\n",
    "Required shared variables:\n",
    "- `AZURE_OPENAI_ENDPOINT` (e.g. https://my-foundry-endpoint.openai.azure.com)\n",
    "- `AZURE_OPENAI_API_VERSION` (legacy fallback only; ignored in unified mode unless you flip the flag)\n",
    "\n",
    "Per-model deployment variables follow: `AZURE_<MODEL_NAME>_MODEL`.\n",
    "Examples:\n",
    "- `AZURE_GPT5_MODEL=your-gpt5-deployment`\n",
    "- `AZURE_GPT4O_MODEL=your-gpt4o-deployment`\n",
    "- `AZURE_GPT41_MODEL=your-gpt-4.1-deployment`\n",
    "\n",
    "Unified Responses API Mode (default):\n",
    "- URL: `POST {AZURE_OPENAI_ENDPOINT}/openai/v1/responses`\n",
    "- Body includes: `{ \"model\": \"<deployment_name>\", \"input\": ... }`\n",
    "\n",
    "Legacy Mode (`USE_LEGACY_DEPLOYMENT_PATH=True`):\n",
    "- URL: `POST {AZURE_OPENAI_ENDPOINT}/openai/deployments/<deployment_name>/responses?api-version=<AZURE_OPENAI_API_VERSION>`\n",
    "- Body may omit `model` (we still include it harmlessly for consistency).\n",
    "\n",
    "Scope used for token acquisition: `https://cognitiveservices.azure.com/.default`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a12e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Configuration Cell\n",
    "# Adjust MODEL_PROFILES to specify mode and reasoning parameters per model.\n",
    "# mode: 'chat' or 'reasoning'\n",
    "# effort: reasoning effort level ('low','medium','high') when mode == 'reasoning'\n",
    "# max_reasoning_tokens: optional cap for reasoning token generation\n",
    "# NOTE: The modern Responses API format uses POST /openai/v1/responses with a 'model' field.\n",
    "# Set USE_LEGACY_DEPLOYMENT_PATH=True to fall back to older deployment-scoped endpoint shape.\n",
    "USE_LEGACY_DEPLOYMENT_PATH = False\n",
    "\n",
    "MODEL_PROFILES = {\n",
    "    \"GPT5\": {\"mode\": \"reasoning\", \"effort\": \"medium\", \"max_reasoning_tokens\": 12000},\n",
    "    \"GPT5MINI\": {\"mode\": \"reasoning\", \"effort\": \"low\", \"max_reasoning_tokens\": 8000},\n",
    "    \"GPT5CHAT\": {\"mode\": \"chat\"},\n",
    "    \"GPT4O\": {\"mode\": \"chat\"},\n",
    "    \"GPT41\": {\"mode\": \"chat\"},\n",
    "}\n",
    "\n",
    "# Backward compatibility: derive MODEL_NAMES list\n",
    "MODEL_NAMES = list(MODEL_PROFILES.keys())\n",
    "\n",
    "PROMPT = (\"Explain the principle of least action in classical mechanics in 3 concise bullet points.\")\n",
    "MAX_OUTPUT_TOKENS = 512  # Set None to omit\n",
    "TEMPERATURE = 0.2\n",
    "REQUEST_TIMEOUT_SECONDS = 60\n",
    "RETRIES = 2  # Additional attempts after the first (total attempts = 1 + RETRIES)\n",
    "PARALLEL = True  # Set False for sequential (easier debugging)\n",
    "MAX_IN_FLIGHT = None  # Optionally cap parallel concurrency; None = len(MODEL_NAMES)\n",
    "EXPORT_RESULTS_JSON = True  # Write results_<timestamp>.json after run\n",
    "SHOW_FULL_TEXT = False  # If True, will display full responses per model (can be verbose)\n",
    "RANK_BY = 'total_tokens'  # Field used for ranking (must exist in record)\n",
    "\n",
    "# Validation set for reasoning effort\n",
    "VALID_REASONING_EFFORT = {\"low\", \"medium\", \"high\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408b27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, dependency checks, and .env loading\n",
    "import os, json, time, math, asyncio, datetime, textwrap\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv  # type: ignore\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass  # It's fine if python-dotenv is not present; env vars may already be set\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import aiohttp\n",
    "try:\n",
    "    from rich.table import Table\n",
    "    from rich.console import Console\n",
    "    from rich import box\n",
    "    console = Console()\n",
    "    HAVE_RICH = True\n",
    "except Exception:\n",
    "    HAVE_RICH = False\n",
    "    console = None\n",
    "    print(\"[WARN] rich not available; falling back to plain-text output.\")\n",
    "\n",
    "SCOPE = \"https://cognitiveservices.azure.com/.default\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455bbfd",
   "metadata": {},
   "source": [
    "## Chat vs Reasoning Modes\n",
    "This notebook now supports both standard chat-style invocations and reasoning-capable models in the same run.\n",
    "\n",
    "Configuration:\n",
    "- Each entry in `MODEL_PROFILES` declares a logical model name and a profile.\n",
    "- `mode`: `chat` sends a normal Responses API request without a `reasoning` block.\n",
    "- `mode`: `reasoning` adds a `reasoning` object to the payload.\n",
    "- `effort`: One of `low|medium|high` (default `medium` if invalid or omitted).\n",
    "- `max_reasoning_tokens`: Optional integer to cap reasoning token generation.\n",
    "\n",
    "Behavior:\n",
    "- Results table will include separate `Mode` and `Effort` columns.\n",
    "- If reasoning was requested but the service returns no reasoning tokens, the row will show `0 ⚠️` in the Reasoning column and a note will be added.\n",
    "- Token ranking still uses `total_tokens` unless you change `RANK_BY`.\n",
    "\n",
    "Adjust `MODEL_PROFILES` to experiment with different mixes of reasoning and chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e18346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://admin-mdrh8xul-eastus2.openai.azure.com/openai/v1/\n",
      "API Version: 2025-04-01-preview\n",
      "Models resolved: {'GPT5': 'gpt-5', 'GPT5MINI': 'gpt-5-mini', 'GPT5CHAT': 'gpt-5-chat', 'GPT4O': 'gpt-4o', 'GPT41': 'gpt-4.1'}\n"
     ]
    }
   ],
   "source": [
    "# Validate Environment Variables and Build Model Deployment Mapping\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "missing = []\n",
    "if not endpoint: missing.append('AZURE_OPENAI_ENDPOINT')\n",
    "if not api_version: missing.append('AZURE_OPENAI_API_VERSION')\n",
    "model_deployments = {}  # model_name -> deployment id\n",
    "for name in MODEL_NAMES:\n",
    "    env_var = f'AZURE_{name.upper()}_MODEL'\n",
    "    val = os.getenv(env_var)\n",
    "    if not val:\n",
    "        missing.append(env_var)\n",
    "    else:\n",
    "        model_deployments[name] = val\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f'Missing required environment variables: {missing}')\n",
    "\n",
    "print('Endpoint:', endpoint)\n",
    "print('API Version:', api_version)\n",
    "print('Models resolved:', model_deployments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19524228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired Azure AD token (truncated): eyJ0eXAiOiJKV1QiLCJhbGci...\n"
     ]
    }
   ],
   "source": [
    "# Credential & Token Handling (simple cache)\n",
    "credential = DefaultAzureCredential()\n",
    "_token_cache = { 'value': None, 'expires_on': 0 }\n",
    "\n",
    "def get_bearer_token(force: bool = False) -> str:\n",
    "    now = time.time()\n",
    "    if (not force) and _token_cache['value'] and now < _token_cache['expires_on'] - 60:\n",
    "        return _token_cache['value']\n",
    "    token = credential.get_token(SCOPE)\n",
    "    _token_cache['value'] = token.token\n",
    "    # azure-identity returns expires_on in epoch seconds attribute\n",
    "    _token_cache['expires_on'] = getattr(token, 'expires_on', now + 600)\n",
    "    return _token_cache['value']\n",
    "\n",
    "# Quick smoke test\n",
    "_ = get_bearer_token()\n",
    "print('Acquired Azure AD token (truncated):', _[:24] + '...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc40ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def build_url(deployment: str) -> str:\n",
    "    base = endpoint.rstrip('/')\n",
    "    if USE_LEGACY_DEPLOYMENT_PATH:\n",
    "        # Legacy style: /openai/deployments/{deployment}/responses?api-version=<version>\n",
    "        return f\"{base}/openai/deployments/{deployment}/responses?api-version={api_version}\"\n",
    "    # Modern unified Responses API endpoint (model specified in body)\n",
    "    if base.endswith('/openai/v1') or base.endswith('/openai/v1/'):\n",
    "        return f\"{base.rstrip('/')}/responses\"\n",
    "    if base.endswith('/openai') or base.endswith('/openai/'):\n",
    "        return f\"{base.rstrip('/')}/v1/responses\"\n",
    "    # Assume base is resource root like https://xyz.openai.azure.com\n",
    "    return f\"{base}/openai/v1/responses\"\n",
    "\n",
    "def truncate(text: Optional[str], limit: int = 120) -> Optional[str]:\n",
    "    if text is None: return None\n",
    "    if len(text) <= limit: return text\n",
    "    return text[:limit-3] + '...'\n",
    "\n",
    "def build_body(profile: Dict[str, Any], deployment: str) -> Dict[str, Any]:\n",
    "    # In modern mode we include model; in legacy mode including it is harmless\n",
    "    body: Dict[str, Any] = {\n",
    "        'model': deployment,\n",
    "        'input': [ { 'role': 'user', 'content': [ { 'type': 'text', 'text': PROMPT } ] } ],\n",
    "        'temperature': TEMPERATURE,\n",
    "    }\n",
    "    if MAX_OUTPUT_TOKENS is not None:\n",
    "        body['max_output_tokens'] = MAX_OUTPUT_TOKENS\n",
    "    mode = profile.get('mode', 'chat')\n",
    "    if mode == 'reasoning':\n",
    "        effort = profile.get('effort', 'medium')\n",
    "        if effort not in VALID_REASONING_EFFORT:\n",
    "            profile.setdefault('notes', []).append(f'invalid_effort:{effort}->medium')\n",
    "            effort = 'medium'\n",
    "        reasoning_obj: Dict[str, Any] = {'effort': effort}\n",
    "        mrt = profile.get('max_reasoning_tokens')\n",
    "        if isinstance(mrt, int) and mrt > 0:\n",
    "            reasoning_obj['max_reasoning_tokens'] = mrt\n",
    "        body['reasoning'] = reasoning_obj\n",
    "    return body\n",
    "\n",
    "def extract_usage(obj: Dict[str, Any]) -> Dict[str, Optional[int]]:\n",
    "    usage = obj.get('usage') or {}\n",
    "    # Direct & legacy fields\n",
    "    input_tokens = usage.get('input_tokens') or usage.get('prompt_tokens')\n",
    "    output_tokens = usage.get('output_tokens') or usage.get('completion_tokens')\n",
    "    # Reasoning tokens may appear in multiple places\n",
    "    reasoning_tokens = (\n",
    "        usage.get('reasoning_tokens')\n",
    "        or (usage.get('output_tokens_details') or {}).get('reasoning_tokens')\n",
    "        or usage.get('output_reasoning_tokens')\n",
    "        or usage.get('output_tokens_reasoning')\n",
    "    )\n",
    "    # Older nested input tokens details variant\n",
    "    if (not reasoning_tokens) and isinstance(usage.get('input_tokens_details'), dict):\n",
    "        reasoning_tokens = usage['input_tokens_details'].get('reasoning_tokens')\n",
    "    total_tokens = usage.get('total_tokens')\n",
    "    if total_tokens is None and (input_tokens is not None or output_tokens is not None):\n",
    "        total_tokens = (input_tokens or 0) + (output_tokens or 0) + (reasoning_tokens or 0)\n",
    "    return {\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'reasoning_tokens': reasoning_tokens,\n",
    "        'total_tokens': total_tokens\n",
    "    }\n",
    "\n",
    "def extract_text(obj: Dict[str, Any]) -> Optional[str]:\n",
    "    # Attempt common shapes for Responses API\n",
    "    if 'output' in obj and isinstance(obj['output'], list):\n",
    "        try:\n",
    "            first = obj['output'][0]\n",
    "            if isinstance(first, dict) and 'content' in first and isinstance(first['content'], list):\n",
    "                seg = first['content'][0]\n",
    "                if isinstance(seg, dict):\n",
    "                    return seg.get('text') or seg.get('value')\n",
    "        except Exception:\n",
    "            pass\n",
    "    if 'choices' in obj and isinstance(obj['choices'], list):  # fallback pattern\n",
    "        try:\n",
    "            ch = obj['choices'][0]\n",
    "            msg = ch.get('message') or {}\n",
    "            if 'content' in msg and isinstance(msg['content'], list):\n",
    "                part = msg['content'][0]\n",
    "                if isinstance(part, dict):\n",
    "                    return part.get('text') or part.get('value')\n",
    "            if isinstance(msg.get('content'), str):\n",
    "                return msg['content']\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Some responses include output_text shortcut\n",
    "    if 'output_text' in obj and isinstance(obj['output_text'], str):\n",
    "        return obj['output_text']\n",
    "    return None\n",
    "\n",
    "async def request_model(session: aiohttp.ClientSession, model_name: str, deployment: str, profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    url = build_url(deployment)\n",
    "    body = build_body(profile, deployment)\n",
    "    attempt = 0\n",
    "    start_time = time.perf_counter()\n",
    "    last_error = None\n",
    "    mode = profile.get('mode', 'chat')\n",
    "    effort = profile.get('effort') if mode == 'reasoning' else None\n",
    "    notes: List[str] = profile.get('notes', [])\n",
    "    debug_prefix = '[LEGACY]' if USE_LEGACY_DEPLOYMENT_PATH else '[UNIFIED]'\n",
    "    print(f\"{debug_prefix} Requesting {model_name} -> model={deployment} endpoint={url}\")\n",
    "    while attempt <= RETRIES:\n",
    "        attempt += 1\n",
    "        token = get_bearer_token()\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {token}',\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        try:\n",
    "            timeout = aiohttp.ClientTimeout(total=REQUEST_TIMEOUT_SECONDS)\n",
    "            async with session.post(url, headers=headers, json=body, timeout=timeout) as resp:\n",
    "                status = resp.status\n",
    "                data = await resp.json(content_type=None)\n",
    "                if status >= 500 or status == 429:\n",
    "                    last_error = f'Status {status}: {data}'\n",
    "                    if attempt <= RETRIES + 0:\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                if status >= 400:\n",
    "                    end = time.perf_counter()\n",
    "                    return {\n",
    "                        'model_name': model_name, 'deployment': deployment, 'status': 'error',\n",
    "                        'http_status': status, 'latency_ms': (end - start_time) * 1000,\n",
    "                        'input_tokens': None, 'output_tokens': None, 'reasoning_tokens': None, 'total_tokens': None,\n",
    "                        'response_excerpt': None, 'error': str(data),\n",
    "                        'mode': mode, 'reasoning_effort': effort, 'reasoning_enabled': mode=='reasoning', 'notes': notes\n",
    "                    }\n",
    "                usage = extract_usage(data)\n",
    "                text = extract_text(data)\n",
    "                end = time.perf_counter()\n",
    "                if mode == 'reasoning' and not usage.get('reasoning_tokens'):\n",
    "                    notes.append('reasoning_tokens_missing')\n",
    "                return {\n",
    "                    'model_name': model_name, 'deployment': deployment, 'status': 'ok',\n",
    "                    'http_status': status, 'latency_ms': (end - start_time) * 1000,\n",
    "                    **usage, 'response_excerpt': truncate(text), 'error': None,\n",
    "                    'mode': mode, 'reasoning_effort': effort, 'reasoning_enabled': mode=='reasoning', 'notes': notes\n",
    "                }\n",
    "        except asyncio.TimeoutError:\n",
    "            last_error = 'timeout'\n",
    "        except Exception as ex:\n",
    "            last_error = repr(ex)\n",
    "        if attempt <= RETRIES + 0:\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    end = time.perf_counter()\n",
    "    return {\n",
    "        'model_name': model_name, 'deployment': deployment, 'status': 'error',\n",
    "        'http_status': None, 'latency_ms': (end - start_time) * 1000,\n",
    "        'input_tokens': None, 'output_tokens': None, 'reasoning_tokens': None, 'total_tokens': None,\n",
    "        'response_excerpt': None, 'error': last_error,\n",
    "        'mode': mode, 'reasoning_effort': effort, 'reasoning_enabled': mode=='reasoning', 'notes': notes\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c2d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting GPT5 at https://admin-mdrh8xul-eastus2.openai.azure.com/openai/deployments/gpt-5/responses?api-version=2025-04-01-preview\n",
      "Requesting GPT5MINI at https://admin-mdrh8xul-eastus2.openai.azure.com/openai/deployments/gpt-5-mini/responses?api-version=2025-04-01-preview\n",
      "Requesting GPT5CHAT at https://admin-mdrh8xul-eastus2.openai.azure.com/openai/deployments/gpt-5-chat/responses?api-version=2025-04-01-preview\n",
      "Requesting GPT4O at https://admin-mdrh8xul-eastus2.openai.azure.com/openai/deployments/gpt-4o/responses?api-version=2025-04-01-preview\n",
      "Requesting GPT41 at https://admin-mdrh8xul-eastus2.openai.azure.com/openai/deployments/gpt-4.1/responses?api-version=2025-04-01-preview\n",
      "Raw Results:\n",
      "[\n",
      "  {\n",
      "    \"model_name\": \"GPT5CHAT\",\n",
      "    \"deployment\": \"gpt-5-chat\",\n",
      "    \"status\": \"error\",\n",
      "    \"http_status\": 404,\n",
      "    \"latency_ms\": 376.4356250030687,\n",
      "    \"input_tokens\": null,\n",
      "    \"output_tokens\": null,\n",
      "    \"reasoning_tokens\": null,\n",
      "    \"total_tokens\": null,\n",
      "    \"response_excerpt\": null,\n",
      "    \"error\": \"{'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "    \"mode\": \"chat\",\n",
      "    \"reasoning_effort\": null,\n",
      "    \"reasoning_enabled\": false,\n",
      "    \"notes\": []\n",
      "  },\n",
      "  {\n",
      "    \"model_name\": \"GPT4O\",\n",
      "    \"deployment\": \"gpt-4o\",\n",
      "    \"status\": \"error\",\n",
      "    \"http_status\": 404,\n",
      "    \"latency_ms\": 376.2643750087591,\n",
      "    \"input_tokens\": null,\n",
      "    \"output_tokens\": null,\n",
      "    \"reasoning_tokens\": null,\n",
      "    \"total_tokens\": null,\n",
      "    \"response_excerpt\": null,\n",
      "    \"error\": \"{'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "    \"mode\": \"chat\",\n",
      "    \"reasoning_effort\": null,\n",
      "    \"reasoning_enabled\": false,\n",
      "    \"notes\": []\n",
      "  },\n",
      "  {\n",
      "    \"model_name\": \"GPT5MINI\",\n",
      "    \"deployment\": \"gpt-5-mini\",\n",
      "    \"status\": \"error\",\n",
      "    \"http_status\": 404,\n",
      "    \"latency_ms\": 381.51541698607616,\n",
      "    \"input_tokens\": null,\n",
      "    \"output_tokens\": null,\n",
      "    \"reasoning_tokens\": null,\n",
      "    \"total_tokens\": null,\n",
      "    \"response_excerpt\": null,\n",
      "    \"error\": \"{'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "    \"mode\": \"reasoning\",\n",
      "    \"reasoning_effort\": \"minimal\",\n",
      "    \"reasoning_enabled\": true,\n",
      "    \"notes\": [\n",
      "      \"invalid_effort:minimal->medium\",\n",
      "      \"invalid_effort:minimal->medium\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"model_name\": \"GPT41\",\n",
      "    \"deployment\": \"gpt-4.1\",\n",
      "    \"status\": \"error\",\n",
      "    \"http_status\": 404,\n",
      "    \"latency_ms\": 381.0474170022644,\n",
      "    \"input_tokens\": null,\n",
      "    \"output_tokens\": null,\n",
      "    \"reasoning_tokens\": null,\n",
      "    \"total_tokens\": null,\n",
      "    \"response_excerpt\": null,\n",
      "    \"error\": \"{'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "    \"mode\": \"chat\",\n",
      "    \"reasoning_effort\": null,\n",
      "    \"reasoning_enabled\": false,\n",
      "    \"notes\": []\n",
      "  },\n",
      "  {\n",
      "    \"model_name\": \"GPT5\",\n",
      "    \"deployment\": \"gpt-5\",\n",
      "    \"status\": \"error\",\n",
      "    \"http_status\": 404,\n",
      "    \"latency_ms\": 434.24875001073815,\n",
      "    \"input_tokens\": null,\n",
      "    \"output_tokens\": null,\n",
      "    \"reasoning_tokens\": null,\n",
      "    \"total_tokens\": null,\n",
      "    \"response_excerpt\": null,\n",
      "    \"error\": \"{'error': {'code': '404', 'message': 'Resource not found'}}\",\n",
      "    \"mode\": \"reasoning\",\n",
      "    \"reasoning_effort\": \"minimal\",\n",
      "    \"reasoning_enabled\": true,\n",
      "    \"notes\": [\n",
      "      \"invalid_effort:minimal->medium\",\n",
      "      \"invalid_effort:minimal->medium\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Orchestration to Run All Models (event-loop safe)\n",
    "async def run_all() -> List[Dict[str, Any]]:\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    semaphore = asyncio.Semaphore(MAX_IN_FLIGHT or len(model_deployments))\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async def run_one(name, dep, profile):\n",
    "            async with semaphore:\n",
    "                return await request_model(session, name, dep, profile)\n",
    "        items = [(n, model_deployments[n], MODEL_PROFILES.get(n, {'mode':'chat'})) for n in MODEL_NAMES if n in model_deployments]\n",
    "        if PARALLEL:\n",
    "            tasks = [asyncio.create_task(run_one(n, d, p)) for n, d, p in items]\n",
    "            for t in asyncio.as_completed(tasks):\n",
    "                res = await t\n",
    "                results.append(res)\n",
    "        else:\n",
    "            for n, d, p in items:\n",
    "                res = await run_one(n, d, p)\n",
    "                results.append(res)\n",
    "    return results\n",
    "\n",
    "# Run with compatibility for existing event loop environments (e.g., Jupyter)\n",
    "try:\n",
    "    import nest_asyncio  # type: ignore\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    # We are inside a Jupyter / IPython environment with a running loop\n",
    "    if not globals().get('_MODEL_COMPARISON_ALREADY_RAN'):\n",
    "        # Use asyncio.create_task + gather pattern\n",
    "        loop = asyncio.get_event_loop()\n",
    "        results = loop.run_until_complete(run_all()) if not loop.is_running() else (await run_all())  # type: ignore\n",
    "        _MODEL_COMPARISON_ALREADY_RAN = True\n",
    "    else:\n",
    "        results = await run_all()  # type: ignore\n",
    "else:\n",
    "    # Standard Python execution path\n",
    "    results = asyncio.run(run_all())\n",
    "\n",
    "print('Raw Results:')\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330553ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing, Ranking, and Reporting\n",
    "def compute_rankings(recs: List[Dict[str, Any]], key: str = RANK_BY) -> List[Dict[str, Any]]:\n",
    "    valid = [r for r in recs if r.get(key) is not None and r['status']=='ok']\n",
    "    valid_sorted = sorted(valid, key=lambda r: r.get(key))\n",
    "    rank_map = {id(r): idx+1 for idx, r in enumerate(valid_sorted)}\n",
    "    best_total = valid_sorted[0].get(key) if valid_sorted else None\n",
    "    for r in recs:\n",
    "        if id(r) in rank_map:\n",
    "            r['rank'] = rank_map[id(r)]\n",
    "            if best_total and r.get(key) is not None:\n",
    "                r['delta_pct'] = ((r.get(key) - best_total)/best_total)*100 if best_total > 0 else 0\n",
    "            else:\n",
    "                r['delta_pct'] = None\n",
    "        else:\n",
    "            r['rank'] = None\n",
    "            r['delta_pct'] = None\n",
    "    return recs\n",
    "\n",
    "results = compute_rankings(results)\n",
    "\n",
    "def render_table(recs: List[Dict[str, Any]]):\n",
    "    if HAVE_RICH:\n",
    "        table = Table(title='Model Comparison', box=box.MINIMAL_DOUBLE_HEAD)\n",
    "        cols = ['Rank','Model','Mode','Effort','Input','Output','Reasoning','Total','Latency(ms)','ΔTotal%','Status','Excerpt']\n",
    "        for c in cols:\n",
    "            table.add_column(c)\n",
    "        ok_with_total = [r for r in recs if r['status']=='ok' and r.get('total_tokens') is not None]\n",
    "        best_total = min([r['total_tokens'] for r in ok_with_total], default=None)\n",
    "        worst_total = max([r['total_tokens'] for r in ok_with_total], default=None)\n",
    "        for r in sorted(recs, key=lambda x: (x['rank'] if x['rank'] is not None else 1e9)):\n",
    "            def fmt(v): return '' if v is None else str(v)\n",
    "            reasoning = r.get('reasoning_tokens')\n",
    "            reasoning_enabled = r.get('reasoning_enabled')\n",
    "            notes = r.get('notes') or []\n",
    "            if reasoning is not None and reasoning > 0:\n",
    "                reasoning_str = f\"{reasoning} 🧠\"\n",
    "            else:\n",
    "                if reasoning_enabled and 'reasoning_tokens_missing' in notes:\n",
    "                    reasoning_str = '0 ⚠️'\n",
    "                else:\n",
    "                    reasoning_str = fmt(reasoning)\n",
    "            total = r.get('total_tokens')\n",
    "            total_str = fmt(total)\n",
    "            style = None\n",
    "            if r['status'] != 'ok':\n",
    "                style = 'red'\n",
    "            elif best_total is not None and total == best_total:\n",
    "                total_str = f\"[bold green]{total} ✅[/]\"\n",
    "            elif worst_total is not None and total == worst_total:\n",
    "                total_str = f\"[bold red]{total} ❌[/]\"\n",
    "            delta = r.get('delta_pct')\n",
    "            delta_str = '' if delta is None else f\"{delta:+.1f}%\"\n",
    "            latency_str = f\"{r.get('latency_ms'):.1f}\" if r.get('latency_ms') is not None else ''\n",
    "            row = [\n",
    "                fmt(r.get('rank')),\n",
    "                r['model_name'],\n",
    "                r.get('mode') or '',\n",
    "                r.get('reasoning_effort') or '',\n",
    "                fmt(r.get('input_tokens')),\n",
    "                fmt(r.get('output_tokens')),\n",
    "                reasoning_str,\n",
    "                total_str,\n",
    "                latency_str,\n",
    "                delta_str,\n",
    "                'OK' if r['status']=='ok' else 'ERROR',\n",
    "                r.get('response_excerpt') or (r.get('error') or '')\n",
    "            ]\n",
    "            table.add_row(*row, style=style)\n",
    "        console.print(table)\n",
    "        ok_latencies = [r['latency_ms'] for r in recs if r['status']=='ok' and r.get('latency_ms') is not None]\n",
    "        if ok_latencies:\n",
    "            console.print(f\"Average latency (ok only): {sum(ok_latencies)/len(ok_latencies):.1f} ms\")\n",
    "        missing_usage = [r['model_name'] for r in recs if r['status']=='ok' and r.get('total_tokens') is None]\n",
    "        if missing_usage:\n",
    "            console.print(f\"[yellow]Warning: Missing token usage for: {missing_usage}[/]\")\n",
    "    else:\n",
    "        print('Rank | Model | Mode | Effort | Input | Output | Reasoning | Total | Latency(ms) | ΔTotal% | Status | Excerpt/Error')\n",
    "        for r in recs:\n",
    "            print(\n",
    "                f\"{r.get('rank')} | {r['model_name']} | {r.get('mode')} | {r.get('reasoning_effort')} | \"\n",
    "                f\"{r.get('input_tokens')} | {r.get('output_tokens')} | {r.get('reasoning_tokens')} | {r.get('total_tokens')} | \"\n",
    "                f\"{r.get('latency_ms') and round(r.get('latency_ms'),1)} | {r.get('delta_pct')} | {r['status']} | \"\n",
    "                f\"{r.get('response_excerpt') or r.get('error')}\"\n",
    "            )\n",
    "\n",
    "render_table(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed0a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results (if enabled)\n",
    "if EXPORT_RESULTS_JSON:\n",
    "    ts = datetime.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    fname = f'results_{ts}.json'\n",
    "    with open(fname, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print('Wrote', fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba7417f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2942d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7dbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23271663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000497bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76dd652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960f604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae94b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundry-model-testing (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
