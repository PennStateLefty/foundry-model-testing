{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6693b79",
   "metadata": {},
   "source": [
    "# Responses API Testing\n",
    "\n",
    "This notebook exercises the Responses API available with Azure OpenAI. It begins with getting a Entra ID token so Managed Identity can be used to secure the calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b114164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports, environment, and credential\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8081cb",
   "metadata": {},
   "source": [
    "## Test stateless using the SDK and Encrypted Reasoning Tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d190307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "reasoning {'id': 'rs_68d3f3a232c881908757f1ea2ec96a3300795284ea9c6894', 'summary': [{'text': \"**Explaining transformer architecture**\\n\\nI need to explain transformer architecture for modern language models in a way that a sixth grader can grasp. I’ll keep things simple, possibly using bullet points and analogies. Tokens can be like LEGO bricks, while embeddings work like coordinates. I’ll illustrate attention with examples, like selecting important words in a classroom setting, and position encoding can be compared to page numbers or music beats. I'll mention why it's powerful: it works parallelly, understands long-range relationships, and scales well with GPUs.\", 'type': 'summary_text'}, {'text': \"**Describing self-attention in Transformers**\\n\\nI want to explain self-attention using analogies like a spotlight or gossip network. I can simplify queries, keys, and values as a question, label, and info. Multi-head attention is like several spotlights. I'll describe residual connections as skip roads and feed-forward networks as little brains refining data. Positional encoding can be seen as rhythm patterns. Training involves predicting the next word and backpropagation like coaching. I'll also list the steps: split text into tokens, convert them into numbers, add positions, and then go through multiple layers to predict the next token.\", 'type': 'summary_text'}, {'text': '**Using analogies to explain LLMs**\\n\\nI want to simplify concepts about language models using analogies. First, I can compare word meanings to a library and highlight the connections between words with magnets. Multi-head attention can be seen as different colored highlighters, with weights as adjustable knobs. During training, the model learns by guessing the next word from many examples, getting corrected, and adjusting weights. I should mention layers as \"pancakes\" and heads as \"eyes.\" I’ll use a simple example where the model pays attention to \"balloon\" to choose \"air\" instead of \"helium.\" Also, GPUs can be likened to super-fast calculators!', 'type': 'summary_text'}, {'text': \"**Crafting an accessible explanation**\\n\\nI want to clearly explain key concepts of transformer architecture for language models. I need to mention tokens, embeddings, positional encoding, and self-attention in simple terms. I can simplify queries, keys, and values while avoiding heavy math and formatting, using bullet points instead for clarity. It’s important to keep this accessible for a sixth grader while providing enough detail. I'll use analogies like a team highlighting important words in a sentence, and introduce the big picture: LLMs as word predictors powered by Transformers, which learn by reading and guessing the next word.\", 'type': 'summary_text'}, {'text': \"**Creating a simple explanation of Transformers**\\n\\nI’ll explain some key concepts related to Transformers in a way that's easy for a sixth grader to grasp. \\n\\n- Tokens are chunks of words.\\n- Embeddings are like ID badges that give each token a number code.\\n- Position info tells us where the word is in a sentence, similar to seat numbers. \\n\\nSelf-attention is when each word looks around for helpful words, using spotlights through multi-heads that focus on various patterns. After this, feed-forward processes refine each word. I’ll also talk about skipping layers to keep things balanced. \\n\\nThen, I’ll cover how the model guesses the next word and learns from its mistakes. Finally, I’ll provide a mini step-by-step example using a simple analogy.\", 'type': 'summary_text'}], 'type': 'reasoning', 'encrypted_content': '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'}\n",
      "message {'id': 'msg_68d3f3c0d5388190852bc90d7f86e3e200795284ea9c6894', 'content': [{'annotations': [], 'text': 'Imagine you’re trying to write a story, one word at a time, and you want each new word to make sense with everything you’ve already written. A transformer is the “thinking engine” inside modern language models that helps with that. Here’s how it works, in kid-friendly terms:\\n\\n- Break words into pieces: The model doesn’t read whole words the way we do. It breaks text into small chunks called tokens (like LEGO bricks of words).\\n\\n- Turn tokens into numbers: Each token gets a special number code that captures its meaning (like an ID badge with features). The model also adds a “position tag” so it knows where each token is in the sentence (like seat numbers in a row).\\n\\n- Pay attention like a team of highlighters: For every token, the model looks at all the other tokens in the sentence and decides which ones are important. This is called attention.\\n  - Think of it like shining spotlights: one spotlight might focus on who did the action; another might focus on what object is being talked about; another on how the sentence is structured.\\n  - Example: In “The boy kicked the ball and it flew high,” when choosing what “it” refers to, the model’s attention shines on “ball.”\\n\\n- Many heads, many views: The model has several attention “heads.” Each head looks for different patterns at the same time—like different colored highlighters marking different things (who, what, where, grammar, long-distance connections).\\n\\n- A small fixer after attention: After paying attention, each token goes through a tiny brain (a feed-forward network) that polishes the info it just gathered.\\n\\n- Keep the original plus the new: The model adds the polished result on top of the original info (skip connections) and keeps everything balanced (so nothing gets too loud or too quiet).\\n\\n- Stack a bunch of these steps: It does the attention + polishing step many times in layers. Think of a stack of pancakes—each layer makes the understanding richer.\\n\\n- Pick the next word: At the end, the model turns its final numbers into chances for each possible next token and chooses one. Then it repeats for the next token, and the next, building a sentence.\\n\\nHow it learns:\\n- Practice, practice, practice: The model reads tons of text and plays a game of “guess the next word.” If it guesses wrong, it nudges its many tiny knobs (weights) to do better next time. After doing this millions of times, it gets very good at spotting patterns.\\n\\nWhy transformers are so good:\\n- They can look at all words at once, not just one-by-one, so they catch long-distance connections.\\n- Different heads notice different kinds of patterns.\\n- They run fast on powerful computers, so they can be very big and very smart.\\n\\nIn short: A transformer is like a team of smart highlighters and tiny fixers that repeatedly scan a sentence, figure out which parts matter, combine that information, and use it to choose the next word—over and over—until the whole answer is written.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n",
      "\n",
      "Follow-up Response:\n",
      "reasoning {'id': 'rs_68d3f3cbc934819096f6ad57575a79e200795284ea9c6894', 'summary': [{'text': \"**Simplifying readability approach**\\n\\nI want to avoid complex math formulas and specialized terms. Instead, I'll focus on keeping sentences short and using common words. I’ll also include bullet points to list my choices while keeping the message concise as per the desired verbosity. It’s important I don't claim to have used any tools or metrics if I haven't. Overall, I'll emphasize my use of analogies and limited jargon while ensuring clarity without disclaimers about being an AI.\", 'type': 'summary_text'}, {'text': \"**Providing a brief overview**\\n\\nI’ll politely explain that I can’t share my step-by-step internal reasoning. Instead, I’ll offer a short overview of my approach and why it's suitable for a sixth grader. It's important to choose the right wording that doesn’t come off as producing a chain-of-thought. I'll focus on declining to share specific processes while effectively summarizing what I did instead. It should keep things clear and accessible for the user.\", 'type': 'summary_text'}, {'text': \"**Crafting the explanation**\\n\\nI can’t share detailed internal reasoning, but I’ll outline the approach I took and why it’s suitable for a sixth grader. Here’s a simplified strategy: \\n\\n- I use familiar analogies and clear examples, like LEGO and highlighters, to break down complex concepts.\\n- I define terms in simple language, avoiding jargon and equations.\\n- The explanation will be in short sentences and sequential steps, with examples to illustrate points.\\n- I’ll maintain a positive tone, focusing on making the content accessible. \\n- Lastly, I'll briefly mention why each analogy works without diving into advanced math.\", 'type': 'summary_text'}, {'text': \"**Explaining suitability**\\n\\nI’ll offer to tailor my response for either a younger or older audience if needed. When asked about my reasoning for suitability for a sixth grader, I won’t just summarize; I’ll explain in detail. \\n\\nI know that sixth graders are developing their abstract reasoning skills, so I’ll use concrete metaphors and ensure the vocabulary is appropriate for their grade level, limiting specialized terms for maximum readability. I'll keep my disclaimers about not revealing my internal thought process concise and to the point. Let's finalize this!\", 'type': 'summary_text'}], 'type': 'reasoning', 'encrypted_content': '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'}\n",
      "message {'id': 'msg_68d3f3e8332c8190871844ae7e5cc65500795284ea9c6894', 'content': [{'annotations': [], 'text': 'I can’t share my step-by-step internal reasoning, but here’s the approach I used and why it’s aimed at a sixth-grade level:\\n\\n- Familiar analogies: I mapped complex parts to everyday things (LEGO bricks for tokens, seat numbers for positions, colored highlighters/spotlights for attention, pancakes for layers). Concrete images help middle-schoolers grasp abstract ideas.\\n- Simple vocabulary and short sentences: I avoided jargon and math, defined any necessary terms in plain language, and kept explanations bite-sized.\\n- Clear structure: I used a sequence of steps (break words, add positions, pay attention, polish, stack, pick next word) so the process feels like a story or recipe.\\n- One quick example: I used “it” referring to “ball” to show how attention works in a way that’s easy to visualize.\\n- Limited depth, core ideas only: I covered the main transformer parts (tokens, embeddings/positions, self-attention, multi-heads, feed-forward, residuals, layers, training) without technical detail, keeping it approachable.\\n\\nIf you want it tuned for a younger audience (more story, fewer terms) or older (more technical detail), I can adjust.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "# OpenAI SDK Async Responses API call\n",
    "from rich import print as rprint\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file if present\n",
    "\n",
    "os.environ[\"OPENAI_LOG\"] = \"debug\"   # Options: debug | info | warn | error\n",
    "\n",
    "# We'll stream a slightly longer prompt to observe deltas.\n",
    "inputs = [{ \"role\": \"user\", \"content\": \"Explain the transformer architecture behind modern LLMs in terms a sixth grader could understand.\" }]\n",
    "reasoning_level = {\n",
    "    \"effort\": \"high\",\n",
    "    \"summary\": \"detailed\"\n",
    "}\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = await client.responses.create(\n",
    "    model=os.getenv(\"AZURE_GPT5_MODEL\"),\n",
    "    input=inputs,\n",
    "    stream=False,\n",
    "    reasoning=reasoning_level,\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    store=False,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "for o in response.output:\n",
    "    print(o.type, o.to_dict())\n",
    "\n",
    "# Follow-up question to test context retention\n",
    "inputs.extend(response.output)\n",
    "inputs.append({ \"role\": \"user\", \"content\": \"Can you explain your reasoning for the last answer? What made you think this was suitable for a sixth grader?\" })\n",
    "response_1 = await client.responses.create(\n",
    "    model=os.getenv(\"AZURE_GPT5_MODEL\"),\n",
    "    input=inputs,\n",
    "    stream=False,\n",
    "    reasoning=reasoning_level,\n",
    "    include=[\"reasoning.encrypted_content\"],\n",
    "    store=False,\n",
    ")\n",
    "\n",
    "print(\"\\nFollow-up Response:\")\n",
    "for o in response_1.output:\n",
    "    print(o.type, o.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5cb72",
   "metadata": {},
   "source": [
    "## Raw REST: Stateless Request with Encrypted Reasoning (API Key Auth)\n",
    "This cell demonstrates **two stateless** raw Responses API calls using an API Key.\n",
    "\n",
    "Goal: Show how including `reasoning.encrypted_content` in the first response lets you forward *only* encrypted reasoning artifacts (plus a new question) in a second stateless call to approximate stateful quality—without using `store` or `previous_response_id`.\n",
    "\n",
    "Flow:\n",
    "1. Call 1 (stateless): `store: false`, `include: [\"reasoning.encrypted_content\"]`.\n",
    "2. Extract encrypted reasoning items returned (type = `reasoning_encrypted_content`).\n",
    "3. Call 2 (stateless): Provide those encrypted items + a follow‑up user question in `input`.\n",
    "4. Compare output quality vs the stateful approach (which used `store:true` + `previous_response_id`).\n",
    "\n",
    "Auth header: `api-key: $AZURE_OPENAI_API_KEY`\n",
    "\n",
    "Body fields used:\n",
    "- `model`: deployment name\n",
    "- `input`: array of items; first call includes the user question; second call includes encrypted reasoning items then the follow‑up user question\n",
    "- `reasoning`: effort & summary hints\n",
    "- `include`: reasoning encrypted content (first call only)\n",
    "- `store`: always `false` here (purely stateless)\n",
    "\n",
    "Outputs printed:\n",
    "- Latency & status for both calls\n",
    "- Token usage\n",
    "- Whether encrypted reasoning was returned & reused\n",
    "- Excerpts from each response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 encrypted reasoning item(s)\n",
      "Sample encrypted fragment: gAAAAABo1AtaRh25Mq7xNJq1qvqWX19mocGuTJihCs31qmbEK_XbS8mK1-WD...\n",
      "Call1 Status=200 Latency=64609.4ms Tokens in:23 out:2220 total:2243 Reasoning?=1472\n",
      "Encrypted reasoning items returned: True\n",
      "--- Call1 Excerpt ---\n",
      "Imagine a transformer as a team of super readers that help a computer write and understand text. Here’s the idea in kid-friendly steps:\n",
      "\n",
      "- Tokens: First, the text is broken into small pieces called tokens (like words or parts of words). Think of toke\n",
      "\n",
      "Call2 Status=400 Latency=18040.6ms Tokens in:None out:None total:None Reasoning?=None\n",
      "Encrypted items forwarded: 1\n"
     ]
    }
   ],
   "source": [
    "# Raw REST stateless two-call example leveraging encrypted reasoning (API Key)\n",
    "import os, json, asyncio, time\n",
    "from typing import Any, Dict, List\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "MODEL = os.getenv(\"AZURE_GPT5_MODEL\")\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "if not ENDPOINT or not MODEL or not API_KEY:\n",
    "    raise RuntimeError(\"Missing required env vars: AZURE_OPENAI_ENDPOINT, AZURE_GPT5_MODEL, AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "FIRST_QUESTION = \"Explain the transformer architecture behind modern LLMs in terms a sixth grader could understand.\"\n",
    "FOLLOWUP_QUESTION = \"Why was that explanation appropriate for a sixth grader?\"  # purely stateless follow-up\n",
    "\n",
    "def gather_encrypted_reasoning(output_list: List[Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return a normalized list of encrypted reasoning carrier objects.\n",
    "\n",
    "        Handles shapes:\n",
    "        1. {\"type\": \"encrypted_content\", \"encrypted_content\": \"...\"}\n",
    "        2. {\"type\": \"reasoning\", \"encrypted_content\": \"...\", ...}\n",
    "        3. Nested occurrences inside lists/dicts (future-proof).\n",
    "        Deduplicates by encrypted_content value.\n",
    "    \"\"\"\n",
    "    candidates: List[Dict[str, Any]] = []\n",
    "    for item in output_list or []:\n",
    "        if not isinstance(item, dict):\n",
    "            continue\n",
    "        t = item.get(\"type\")\n",
    "        # Direct encrypted item variants\n",
    "        if t in (\"encrypted_content\", \"reasoning_encrypted_content\") and item.get(\"encrypted_content\"):\n",
    "            candidates.append({\n",
    "                \"type\": t if t != \"reasoning_encrypted_content\" else \"encrypted_content\",\n",
    "                \"encrypted_content\": item[\"encrypted_content\"]\n",
    "            })\n",
    "            continue\n",
    "        # Reasoning wrapper containing encrypted_content\n",
    "        if t == \"reasoning\" and item.get(\"encrypted_content\"):\n",
    "            candidates.append({\n",
    "                \"type\": \"reasoning\",\n",
    "                \"encrypted_content\": item[\"encrypted_content\"]\n",
    "            })\n",
    "        # Deep scan for nested dict/list values that expose 'encrypted_content'\n",
    "        for v in item.values():\n",
    "            if isinstance(v, dict) and v.get(\"encrypted_content\"):\n",
    "                candidates.append({\n",
    "                    \"type\": v.get(\"type\") or \"reasoning\",\n",
    "                    \"encrypted_content\": v[\"encrypted_content\"]\n",
    "                })\n",
    "            elif isinstance(v, list):\n",
    "                for sub in v:\n",
    "                    if isinstance(sub, dict) and sub.get(\"encrypted_content\"):\n",
    "                        candidates.append({\n",
    "                            \"type\": sub.get(\"type\") or \"reasoning\",\n",
    "                            \"encrypted_content\": sub[\"encrypted_content\"]\n",
    "                        })\n",
    "    # Deduplicate by encrypted_content\n",
    "    seen = set()\n",
    "    dedup: List[Dict[str, Any]] = []\n",
    "    for c in candidates:\n",
    "        ec = c.get(\"encrypted_content\")\n",
    "        if ec and ec not in seen:\n",
    "            seen.add(ec)\n",
    "            dedup.append(c)\n",
    "    return dedup\n",
    "\n",
    "async def stateless_with_encrypted_reasoning():\n",
    "    url = f\"{ENDPOINT}/responses\"\n",
    "    headers = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    # ---- Call 1 ----\n",
    "    body1: Dict[str, Any] = {\n",
    "        \"model\": MODEL,\n",
    "        \"input\": [ { \"role\": \"user\", \"content\": FIRST_QUESTION } ],\n",
    "        \"reasoning\": {\"effort\": \"high\", \"summary\": \"detailed\"},\n",
    "        \"include\": [\"reasoning.encrypted_content\"],\n",
    "        \"store\": False\n",
    "    }\n",
    "    start1 = time.perf_counter()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, headers=headers, json=body1) as resp1:\n",
    "            data1 = await resp1.json(content_type=None)\n",
    "            lat1 = (time.perf_counter() - start1) * 1000\n",
    "            status1 = resp1.status\n",
    "    usage1 = data1.get(\"usage\", {})\n",
    "    # NEW: robust extraction\n",
    "    encrypted_items = gather_encrypted_reasoning(data1.get(\"output\"))\n",
    "    encrypted_found = len(encrypted_items) > 0\n",
    "    if not encrypted_found:\n",
    "        print(\"[warn] No encrypted reasoning items found. Output keys: \" +\n",
    "              f\"{[list(o.keys()) for o in data1.get('output', []) if isinstance(o, dict)]}\")\n",
    "    else:\n",
    "        print(f\"Found {len(encrypted_items)} encrypted reasoning item(s)\")\n",
    "    # Optional: show first 60 chars of first token for confirmation (not full content)\n",
    "    if encrypted_found:\n",
    "        print(\"Sample encrypted fragment: \" + encrypted_items[0][\"encrypted_content\"][:60] + \"...\")\n",
    "\n",
    "    excerpt1 = None\n",
    "    for o in data1.get(\"output\", []):\n",
    "        if isinstance(o, dict):\n",
    "            for c in o.get(\"content\", []):\n",
    "                if isinstance(c, dict) and c.get(\"text\"):\n",
    "                    excerpt1 = c[\"text\"][:250]\n",
    "                    break\n",
    "        if excerpt1: break\n",
    "\n",
    "    print(f\"Call1 Status={status1} Latency={lat1:.1f}ms Tokens in:{usage1.get('input_tokens')} out:{usage1.get('output_tokens')} total:{usage1.get('total_tokens')} Reasoning?={(usage1.get('reasoning_tokens') or (usage1.get('output_tokens_details') or {}).get('reasoning_tokens'))}\")\n",
    "    print(f\"Encrypted reasoning items returned: {encrypted_found}\")\n",
    "    if excerpt1:\n",
    "        print(\"--- Call1 Excerpt ---\\n\" + excerpt1 + \"\\n\")\n",
    "\n",
    "    # ---- Call 2 (stateless follow-up) ----\n",
    "    followup_input: List[Any] = []\n",
    "    followup_input.extend(data1.get(\"output\", []))  # forward minimal encrypted carriers\n",
    "    followup_input.append({\"role\": \"user\", \"content\": FOLLOWUP_QUESTION})\n",
    "\n",
    "    body2: Dict[str, Any] = {\n",
    "        \"model\": MODEL,\n",
    "        \"input\": followup_input,\n",
    "        \"reasoning\": {\"effort\": \"high\"},\n",
    "        \"store\": False\n",
    "    }\n",
    "    start2 = time.perf_counter()\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, headers=headers, json=body2) as resp2:\n",
    "            data2 = await resp2.json(content_type=None)\n",
    "            lat2 = (time.perf_counter() - start2) * 1000\n",
    "            status2 = resp2.status\n",
    "    usage2 = data2.get(\"usage\", {})\n",
    "\n",
    "    excerpt2 = None\n",
    "    for o in data2.get(\"output\", []):\n",
    "        if isinstance(o, dict):\n",
    "            for c in o.get(\"content\", []):\n",
    "                if isinstance(c, dict) and c.get(\"text\"):\n",
    "                    excerpt2 = c[\"text\"][:250]\n",
    "                    break\n",
    "        if excerpt2: break\n",
    "\n",
    "    print(f\"Call2 Status={status2} Latency={lat2:.1f}ms Tokens in:{usage2.get('input_tokens')} out:{usage2.get('output_tokens')} total:{usage2.get('total_tokens')} Reasoning?={(usage2.get('reasoning_tokens') or (usage2.get('output_tokens_details') or {}).get('reasoning_tokens'))}\")\n",
    "    print(f\"Encrypted items forwarded: {len(encrypted_items)}\")\n",
    "    if excerpt2:\n",
    "        print(\"--- Call2 Excerpt ---\\n\" + excerpt2 + \"\\n\")\n",
    "\n",
    "    if excerpt1 and excerpt2:\n",
    "        print(f\"Follow-up excerpt length delta: {len(excerpt2) - len(excerpt1)} chars (positive means longer explanation)\")\n",
    "\n",
    "await stateless_with_encrypted_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d1d08",
   "metadata": {},
   "source": [
    "## Test Stateful using the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "130ea90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Response ID: resp_68d3f3e983f88196bd334ffe675d6581013e04e7c4fc8a1b\n",
      "Response:\n",
      "reasoning {'id': 'rs_68d3f3ea21708196ad53df108e4b1c9e013e04e7c4fc8a1b', 'summary': [{'text': \"**Explaining language models**\\n\\nI'm considering mentioning that transformers were originally invented for translation, using both encoders and decoders, but many LLMs today, like GPT, only use a decoder. I could explain tokens and context windows—how much they can remember—like a backpack of recent words. I’ll also briefly touch on key-value caches for speed. But I might simplify the explanation for a 6th grader, presenting parameters as knobs and using relatable examples like layers of pancakes or Lego blocks. I’d clarify how attention scores work using intuitive phrases!\", 'type': 'summary_text'}, {'text': '**Simplifying model concepts**\\n\\nI’m thinking about how to explain concepts like multi-head attention. I could say it\\'s like multiple friends reading the same sentence and focusing on different parts. For feed-forward networks, I might describe them as mini-brains processing each word separately. Positional encodings could be framed as a way to remember order, like adding tags to indicate position.\\n\\nResidual connections might be shown as bridges that help information pass along. I want to keep everything approachable and at a sixth-grade vocabulary level, avoiding complex terms like \"matrix multiplication.\"', 'type': 'summary_text'}, {'text': '**Structuring a story analogy**\\n\\nI want to create an accurate yet simplified analogy for explaining LLMs. First, I can present the big picture as a giant auto-complete machine that reads text and predicts the next word. \\n\\nThen, I’ll chop text into tokens and turn those into embeddings (or numbers) for the computer. I’ll mention positions to explain order and describe transformer blocks as having attention and feed-forward components. \\n\\nAttention could be compared to comparing words, highlighting that it finds connections. Multi-head attention acts like several spotlights, while stacking layers helps deepen understanding. I’ll explain generation as choosing tokens repeatedly, and I should mention training as reading a lot of text and adjusting weights.', 'type': 'summary_text'}, {'text': '**Crafting a friendly explanation**\\n\\nI need to ensure the explanation is suitable for a sixth grader, avoiding heavy formatting while using bullet points for clarity. It would be great to provide simple analogies along with this structure.\\n\\nIt\\'s important to clarify that the model breaks words into tokens, sometimes subwords or characters. I might mention that it doesn\\'t retain facts but rather identifies patterns. I can include examples of ambiguity, like in \"The dog didn\\'t eat the homework because it was full,\" where attention helps determine what “it” refers to. For instance, in \"The ball that John threw hit the window,\" attention looks back at “ball” to predict “hit.”', 'type': 'summary_text'}, {'text': '**Explaining transformer architecture simply**\\n\\nI think using \"spotlights\" as an analogy works well, where each word shines a light on others to help select the next word. For multi-head attention, I can explain it as many colored spotlights. \\n\\nI can outline a step-by-step process: \\n1. Split sentences into tokens. \\n2. Assign numbers and position tags to each token.\\n3. Each token checks all others to choose which to learn from.\\n4. Each token deepens its thinking based on what it gathered.\\n5. Repeat the process multiple times.\\n6. Predict the next token and continue.\\n\\nI\\'ll use approachable examples to clarify concepts, like showing how pronouns work in sentences. I want to be clear that LLMs don’t truly understand; they spot patterns. \\nI’ll describe their training as practice where they adjust based on feedback, ensuring everything remains accessible for a sixth grader.', 'type': 'summary_text'}, {'text': '**Clarifying word prediction**\\n\\nFor this explanation, saying that the model \"picks a likely next word\" works well. I should mention how it uses tokenization to break words into smaller parts when needed, like turning \"unbreakable\" into \"un,\" \"break,\" and \"able.\" I won’t go into edge details or technical terms like \"##\" or BPE since it\\'s more about keeping things straightforward. I\\'ll focus on clarity while making sure it’s engaging for the user. Let\\'s move forward with this!', 'type': 'summary_text'}], 'type': 'reasoning'}\n",
      "message {'id': 'msg_68d3f440d3588196a213ef0706339ffa013e04e7c4fc8a1b', 'content': [{'annotations': [], 'text': 'Imagine a super‑powered auto‑complete. You start a sentence, and it guesses the next word, then the next, until it writes a whole paragraph. A “transformer” is the kind of brain inside that does the guessing.\\n\\nHere’s how it works, in kid-friendly steps:\\n\\n1) Break text into pieces\\n- The model doesn’t see whole words the way we do. It chops text into tiny pieces called tokens (sometimes a whole word, sometimes parts like “un-” or “-ing”).\\n\\n2) Turn pieces into numbers\\n- Computers understand numbers, not letters. So each token gets turned into a list of numbers that capture its meaning (this is called an embedding).\\n- It also gets a “place tag” so the model knows the order of tokens (first, second, third…). That’s how it remembers word order.\\n\\n3) Let every word “pay attention” to the others\\n- The key idea is called self‑attention. Picture each token holding a flashlight and looking around the whole sentence to find the most helpful words.\\n- For example, in “I sat by the bank of the river,” the token “bank” shines its light on “river” to realize “bank” means riverbank, not a money bank.\\n- The model gives stronger “attention” to tokens that matter more for understanding or predicting the next token.\\n\\n4) Use many flashlights at once (multi‑head attention)\\n- Instead of one flashlight, the model uses several in different “colors.” Each one looks for different things—like one might track who did what, another might link adjectives to nouns, another might watch long‑distance connections in the sentence.\\n\\n5) Think a bit more about each token\\n- After attention, each token goes through a small “thinking” step (a tiny neural network) that mixes and refines what it just gathered. This helps it form richer ideas.\\n\\n6) Stack these steps many times\\n- A transformer is built from many layers of “attention + thinking” blocks, one on top of another. Early layers notice simple stuff; later layers spot deeper patterns and meaning.\\n\\n7) Predict the next token\\n- After passing through the stack, the model guesses the next token that best fits. It picks a likely choice, adds it to the text, and repeats the whole process to get the next one, and the next, and so on.\\n\\nHow it learns\\n- Practice: The model reads huge amounts of text (books, websites, articles) and plays “guess the next token.”\\n- Feedback: When it’s wrong, it nudges millions or billions of tiny “knobs” (parameters) to be a bit better next time.\\n- Over time, it gets very good at patterns: grammar, facts it has seen, styles of writing, and how ideas connect.\\n\\nWhy this design is powerful\\n- It looks at all the words at once, so it can connect far‑apart ideas in a sentence or paragraph.\\n- Multiple attention “heads” notice different kinds of relationships at the same time.\\n- Stacking many layers lets it build from simple clues to complex understanding.\\n\\nA few extra notes\\n- Many chatty LLMs use just the “decoder” half of the original transformer design. Think of it as the part that writes text, one token at a time.\\n- The model doesn’t “know” things like a person; it’s excellent at spotting patterns and using them to predict what comes next.\\n- It has a short‑term memory called a context window—it can only pay attention to a certain amount of recent text at once.\\n\\nQuick recap\\n- Tokens: tiny pieces of text\\n- Embeddings + positions: numbers that capture meaning and order\\n- Self‑attention: tokens shine flashlights on the most helpful other tokens\\n- Multi‑head attention: many flashlights looking for different patterns\\n- Feed‑forward “thinking”: refine what was gathered\\n- Stacked layers: build deeper understanding\\n- Predict next token: write text step by step\\n\\nThat’s the transformer: a big stack of attention and thinking blocks that makes smart, pattern‑based guesses about what comes next in language.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n",
      "\n",
      "Follow-up Response:\n",
      "reasoning {'id': 'rs_68d3f4606c248196ac5c964b284c6af9013e04e7c4fc8a1b', 'summary': [{'text': \"**Summarizing readability approach**\\n\\nI'm aiming for simplicity with my vocabulary, using short sentences and analogies like flashlights for clarity. I need to define key terms and maintain a step-by-step structure, targeting a 6th-grade reading level. I'll include examples while avoiding jargon. \\n\\nI think it’s important not to claim precise readability metrics without accurate measurement, but I can describe an approximate readability level. I’ll also mention we avoided heavy formatting and kept a friendly tone throughout.\", 'type': 'summary_text'}, {'text': '**Explaining the approach**\\n\\nI focused on breaking down complex terms using simple analogies: for instance, tokens as pieces, embeddings as numbers, and self-attention as flashlights. I also used relatable examples, like a bank near a river, to make concepts more familiar. The goal was to keep paragraphs small, use plain language, and avoid equations. Additionally, I included a recap and mentioned that I can adjust to simpler or more advanced versions if needed. Overall, I aimed for accessibility through analogies and concrete examples without diving into introspection.', 'type': 'summary_text'}, {'text': '**Explaining my reasoning**\\n\\nI aimed for clarity by choosing simple vocabulary and defining any jargon. I used analogies—like comparing flashlights to attention and tokens to pieces—to make concepts relatable. The structure included step-by-step numbered sections and an example about a bank to illustrate ambiguity, while avoiding math and code. \\n\\nI think a typical 6th grader could understand this since they’re familiar with basic reading and texting. I’m open to making adjustments, like adding visuals or simplifying even further if needed.', 'type': 'summary_text'}, {'text': '**Explaining my suitability rationale**\\n\\nTo address why this approach is suitable for a sixth grader, here’s a high-level overview:\\n\\n1. I used plain language with everyday words and explained the terms clearly.\\n2. The structure is organized into short steps with headings and bullet lists for clarity.\\n3. I incorporated relatable analogies, like comparing self-attention to \"flashlights\" and tokens to \"pieces.\"\\n4. The examples, such as “auto-complete game,” help connect familiar concepts to new ideas.\\n\\nThis keeps the content accessible and engaging for that age group!', 'type': 'summary_text'}, {'text': '**Justifying suitability for sixth graders**\\n\\nI used the \"bank of the river\" versus \"money bank\" example to illustrate context appropriately for sixth graders. My approach focuses on core ideas without diving into math or complex topics like calculus and code. I included engaging examples and a quick recap to aid memory, keeping the content positive and at a simple length. \\n\\nHere’s why it\\'s suitable for sixth graders:\\n1. Familiar with smartphone features and building metaphors.\\n2. Comfortable following sets of instructions and using analogies.\\n3. Can grasp cause-and-effect relationships with minimal prior knowledge.\\n\\nI avoided heavy details like matrix multiplication and gradient descent. I\\'m open to simplifying further or providing a very brief version if needed!', 'type': 'summary_text'}], 'type': 'reasoning'}\n",
      "message {'id': 'msg_68d3f47a265c819698264d4e9492dc5d013e04e7c4fc8a1b', 'content': [{'annotations': [], 'text': 'Here’s a quick, high-level explanation of my approach, without going into step-by-step internal reasoning:\\n\\n- Plain language: I used everyday words and defined any jargon right away (tokens as “pieces,” embeddings as “numbers,” positional info as “place tags”).\\n- Friendly analogies: I turned abstract ideas into familiar pictures—flashlights for attention, different “colors” for multi-head attention, a stack of blocks for layers, and an auto-complete game for how it writes.\\n- Concrete example: The “bank by the river” sentence shows how context changes meaning, which helps a middle-schooler see why attention matters.\\n- Simple structure: Short sections with a clear sequence (break text, turn into numbers, pay attention, stack layers, predict next token), plus a quick recap.\\n- Age-appropriate scope: I avoided math, code, and deep technical terms (like matrices, softmax, or gradient descent) and focused on the core ideas.\\n- Brevity and clarity: Sentences are relatively short, active, and direct to match typical sixth-grade reading comfort.\\n- Relatability: Many sixth graders know auto-complete from phones, so starting there gives them a familiar anchor.\\n\\nIf you want it even simpler or a bit more technical, I can adjust the explanation to match a different level.', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}\n"
     ]
    }
   ],
   "source": [
    "# OpenAI SDK Async Responses API call\n",
    "from rich import print as rprint\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file if present\n",
    "\n",
    "os.environ[\"OPENAI_LOG\"] = \"debug\"   # Options: debug | info | warn | error\n",
    "\n",
    "# We'll stream a slightly longer prompt to observe deltas.\n",
    "inputs = [{ \"role\": \"user\", \"content\": \"Explain the transformer architecture behind modern LLMs in terms a sixth grader could understand.\" }]\n",
    "reasoning_level = {\n",
    "    \"effort\": \"high\",\n",
    "    \"summary\": \"detailed\"\n",
    "}\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = await client.responses.create(\n",
    "    model=os.getenv(\"AZURE_GPT5_MODEL\"),\n",
    "    input=inputs,\n",
    "    stream=False,\n",
    "    reasoning=reasoning_level,\n",
    "    store=True,\n",
    ")\n",
    "\n",
    "previous_response_id = response.id\n",
    "print(f\"Previous Response ID: {previous_response_id}\")\n",
    "\n",
    "print(\"Response:\")\n",
    "for o in response.output:\n",
    "    print(o.type, o.to_dict())\n",
    "\n",
    "# Follow-up question to test context retention\n",
    "inputs.append({ \"role\": \"user\", \"content\": \"Can you explain your reasoning for the last answer? What made you think this was suitable for a sixth grader?\" })\n",
    "response_1 = await client.responses.create(\n",
    "    model=os.getenv(\"AZURE_GPT5_MODEL\"),\n",
    "    input=inputs,\n",
    "    stream=False,\n",
    "    reasoning=reasoning_level,\n",
    "    previous_response_id=previous_response_id,\n",
    "    store=True,\n",
    ")\n",
    "\n",
    "print(\"\\nFollow-up Response:\")\n",
    "for o in response_1.output:\n",
    "    print(o.type, o.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df936c57",
   "metadata": {},
   "source": [
    "## Raw REST: Stateful (Chained) Request (API Key Auth)\n",
    "This section mirrors the *stateful* SDK example using raw HTTP calls with an **API Key**:\n",
    "- First call: `store: true` to persist context; capture `response.id`.\n",
    "- Second call: passes `previous_response_id` to continue the conversation.\n",
    "- Authentication header: `api-key: $AZURE_OPENAI_API_KEY`.\n",
    "- Demonstrates model retaining prior context without resending earlier text (other than the new follow-up prompt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79173eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://admin-mdrh8xul-eastus2.openai.azure.com/openai/v1\n",
      "First call status=200 latency=38315.0 ms id=resp_68d3f72c8a2c8195a0e9af342799c04c04c33f6bfd9c3052\n",
      "--- First output excerpt ---\n",
      "Imagine you’re writing a story one word at a time. A modern LLM (large language model) is a super-fast “guessing machine” that tries to predict the next word. The transformer is the design that helps \n",
      "\n",
      "Follow-up status=200 latency=12814.7 ms id=resp_68d3f752763c8195bd5fe753d370a41604c33f6bfd9c3052\n",
      "--- Follow-up output excerpt ---\n",
      "It was aimed at a sixth grader by keeping ideas simple, concrete, and relatable:\n",
      "\n",
      "- Used everyday analogies: Lego bricks for tokens, secret codes for meanings, seat numbers for word order, and flashlights for attention.\n",
      "- Gave a clear, familiar examp\n",
      "\n",
      "Usage first: in:23 out:1472 total:1495\n",
      "Usage follow-up: in:677 out:512 total:1189\n"
     ]
    }
   ],
   "source": [
    "# Raw REST stateful chained example (API Key)\n",
    "import os, json, asyncio, time\n",
    "from typing import Any, Dict\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\").rstrip(\"/\")\n",
    "print(ENDPOINT)\n",
    "MODEL = os.getenv(\"AZURE_GPT5_MODEL\")\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "if not ENDPOINT or not MODEL or not API_KEY:\n",
    "    raise RuntimeError(\"Missing required env vars: AZURE_OPENAI_ENDPOINT, AZURE_GPT5_MODEL, AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "async def raw_stateful_api_key():\n",
    "    url = f\"{ENDPOINT}/responses\"\n",
    "    headers = {\n",
    "        \"api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    # First request (store=True)\n",
    "    first_body: Dict[str, Any] = {\n",
    "        \"model\": MODEL,\n",
    "        \"input\": \"Explain the transformer architecture behind modern LLMs in terms a sixth grader could understand.\",\n",
    "        \"store\": True,\n",
    "        \"reasoning\": {\"effort\": \"medium\"}\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        t0 = time.perf_counter()\n",
    "        async with session.post(url, headers=headers, json=first_body) as resp1:\n",
    "            data1 = await resp1.json(content_type=None)\n",
    "            latency1 = (time.perf_counter() - t0) * 1000\n",
    "            print(f\"First call status={resp1.status} latency={latency1:.1f} ms id={data1.get('id')}\")\n",
    "        prev_id = data1.get(\"id\")\n",
    "        excerpt = None\n",
    "        for o in data1.get(\"output\", []):\n",
    "            if isinstance(o, dict):\n",
    "                for c in o.get(\"content\", []):\n",
    "                    if isinstance(c, dict) and c.get(\"text\"):\n",
    "                        excerpt = c[\"text\"][:200]\n",
    "                        break\n",
    "            if excerpt:\n",
    "                break\n",
    "        if excerpt:\n",
    "            print(\"--- First output excerpt ---\\n\" + excerpt + \"\\n\")\n",
    "        # Second request using previous_response_id\n",
    "        followup_body: Dict[str, Any] = {\n",
    "            \"model\": MODEL,\n",
    "            \"previous_response_id\": prev_id,\n",
    "            \"input\": \"Why was that explanation appropriate for a sixth grader?\",\n",
    "            \"store\": True\n",
    "        }\n",
    "        t1 = time.perf_counter()\n",
    "        async with session.post(url, headers=headers, json=followup_body) as resp2:\n",
    "            data2 = await resp2.json(content_type=None)\n",
    "            latency2 = (time.perf_counter() - t1) * 1000\n",
    "            print(f\"Follow-up status={resp2.status} latency={latency2:.1f} ms id={data2.get('id')}\")\n",
    "        excerpt2 = None\n",
    "        for o in data2.get(\"output\", []):\n",
    "            if isinstance(o, dict):\n",
    "                for c in o.get(\"content\", []):\n",
    "                    if isinstance(c, dict) and c.get(\"text\"):\n",
    "                        excerpt2 = c[\"text\"][:250]\n",
    "                        break\n",
    "            if excerpt2:\n",
    "                break\n",
    "        if excerpt2:\n",
    "            print(\"--- Follow-up output excerpt ---\\n\" + excerpt2 + \"\\n\")\n",
    "        def usage_line(d):\n",
    "            u = d.get(\"usage\", {})\n",
    "            return f\"in:{u.get('input_tokens')} out:{u.get('output_tokens')} total:{u.get('total_tokens')}\"\n",
    "        print(\"Usage first:\", usage_line(data1))\n",
    "        print(\"Usage follow-up:\", usage_line(data2))\n",
    "\n",
    "await raw_stateful_api_key()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundry-model-testing (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
